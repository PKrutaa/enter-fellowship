# ğŸš€ Sistema de ExtraÃ§Ã£o de Dados de PDFs

Sistema de extraÃ§Ã£o estruturada de dados de documentos PDF com alta acurÃ¡cia, baixa latÃªncia e custo otimizado. Production-ready.

## ğŸ“‹ Tabela de ConteÃºdo

- [InÃ­cio RÃ¡pido com Docker](#-inÃ­cio-rÃ¡pido-com-docker)
- [Processamento em Batch (Sem UI)](#-processamento-em-batch-sem-ui)
- [API REST](#-api-rest)
- [Arquitetura](#-arquitetura)
- [Performance](#-performance)

---

## ğŸ³ InÃ­cio RÃ¡pido com Docker

### PrÃ©-requisitos
- Docker e Docker Compose instalados
- Chave da API OpenAI

### Passo a Passo

**1. Configure a API Key**
```bash
echo "OPENAI_API_KEY=sua-chave-aqui" > .env
```

**2. Inicie os containers**
```bash
docker compose up -d
```

**3. Verifique se estÃ¡ funcionando**
```bash
# Ver logs
docker compose logs -f

# Testar health check
curl http://localhost:8000/health
```

**4. Acesse a API**
- **API**: http://localhost:8000
- **DocumentaÃ§Ã£o**: http://localhost:8000/docs
- **Health**: http://localhost:8000/health
- **Stats**: http://localhost:8000/stats

### Comandos Docker Ãšteis

```bash
# Parar containers
docker compose down

# Rebuild apÃ³s mudanÃ§as
docker compose up -d --build

# Ver logs em tempo real
docker compose logs -f api

# Entrar no container
docker compose exec api bash

# Ver uso de recursos
docker stats

# Limpar tudo (incluindo volumes)
docker compose down -v
```

---

## ğŸ“¦ Processamento em Batch (Sem UI)

### OpÃ§Ã£o 1: Script CLI (Recomendado)

Para processamento offline em lote de um diretÃ³rio:

**Executar o script:**
```bash
# Dentro do container Docker
docker compose exec api python src/batch_extract.py \
  --pdf-dir ai-fellowship-data/files \
  --dataset-path ai-fellowship-data/dataset.json \
  --output-dir output

# Ou localmente (se tiver Python configurado)
python src/batch_extract.py \
  --pdf-dir ai-fellowship-data/files \
  --dataset-path ai-fellowship-data/dataset.json \
  --output-dir output

# Ou utilize uv (recomendado)
uv run src/batch_extract.py \
  --pdf-dir ai-fellowship-data/files \
  --dataset-path ai-fellowship-data/dataset.json \
  --output-dir output
```

**Estrutura esperada do dataset.json:**
```json
[
  {
    "pdf_path": "oab_1.pdf",
    "label": "carteira_oab",
    "extraction_schema": {
      "nome": "Nome completo",
      "inscricao": "NÃºmero de inscriÃ§Ã£o OAB"
    }
  },
  {
    "pdf_path": "tela_sistema_1.pdf",
    "label": "tela_sistema",
    "extraction_schema": {
      "sistema": "Nome do sistema",
      "valor_parcela": "Valor da parcela"
    }
  }
]
```

**SaÃ­da:**
- Cria arquivo `output/consolidated_results.json` com todos os resultados
- Cria arquivos individuais em `output/` para cada PDF processado
- Cada resultado inclui: dados extraÃ­dos, mÃ©todo usado (cache/template/llm), metadata do pipeline

**Exemplo de saÃ­da (`consolidated_results.json`):**
```json
{
  "total_processed": 2,
  "total_success": 2,
  "total_failed": 0,
  "processing_time_seconds": 7.84,
  "results": [
    {
      "pdf_path": "oab_1.pdf",
      "label": "carteira_oab",
      "success": true,
      "data": {
        "nome": "JoÃ£o Silva",
        "inscricao": "123456"
      },
      "metadata": {
        "method": "llm",
        "pipeline_info": {
          "method": "llm",
          "time": 3.62
        }
      }
    },
    {
      "pdf_path": "oab_2.pdf",
      "label": "carteira_oab",
      "success": true,
      "data": {
        "nome": "Maria Santos",
        "inscricao": "789012"
      },
      "metadata": {
        "method": "template",
        "pipeline_info": {
          "method": "template",
          "similarity": 92.5,
          "time": 0.51
        }
      }
    }
  ]
}
```


### OpÃ§Ã£o 2: Via API com Streaming

A API suporta **processamento progressivo com Server-Sent Events (SSE)**:

**CaracterÃ­sticas:**
- âœ… **MÃºltiplos PDFs, mÃºltiplas labels**: Cada arquivo pode ter label e schema diferentes
- âœ… **Processamento paralelo por label**: Labels diferentes processam simultaneamente
- âœ… **Resultados progressivos**: Recebe cada PDF assim que Ã© processado (nÃ£o espera o batch completo)
- âœ… **Template learning**: Documentos do mesmo label processam sequencialmente para aprendizado

**Exemplo Python:**

```python
import requests
import json

# Preparar arquivos e metadados
files_data = [
    {
        "file": ("oab_1.pdf", open("oab_1.pdf", "rb"), "application/pdf"),
        "label": "carteira_oab",
        "schema": {"nome": "Nome completo", "inscricao": "NÃºmero OAB"}
    },
    {
        "file": ("tela_1.pdf", open("tela_1.pdf", "rb"), "application/pdf"),
        "label": "tela_sistema",
        "schema": {"sistema": "Nome do sistema", "valor": "Valor total"}
    },
    {
        "file": ("oab_2.pdf", open("oab_2.pdf", "rb"), "application/pdf"),
        "label": "carteira_oab",
        "schema": {"nome": "Nome completo", "inscricao": "NÃºmero OAB"}
    }
]

# Criar FormData
form_data = []
for item in files_data:
    form_data.append(("files", item["file"]))
    
# Adicionar labels e schemas na mesma ordem
labels = [item["label"] for item in files_data]
schemas = [json.dumps(item["schema"]) for item in files_data]

data = {
    "labels": labels,
    "schemas": schemas
}

# Fazer request com streaming
response = requests.post(
    "http://localhost:8000/extract-batch",
    files=[("files", f[1]) for f in form_data],
    data={
        "labels": labels,
        "schemas": schemas
    },
    stream=True  # ğŸ”¥ Importante: habilita streaming
)

# Processar resultados progressivamente
for line in response.iter_lines(decode_unicode=True):
    if not line:
        continue
        
    if line.startswith("event:"):
        event_type = line.split(":", 1)[1].strip()
    elif line.startswith("data:"):
        data = json.loads(line.split(":", 1)[1].strip())
        
        if event_type == "result":
            # Resultado individual (recebido assim que processa)
            filename = data["filename"]
            success = data["success"]
            method = data["metadata"].get("method", "unknown")
            time = data["metadata"].get("time", 0)
            
            print(f"âœ“ {filename}: {method} ({time:.2f}s)")
            
            if success:
                print(f"  Dados: {data['data']}")
            else:
                print(f"  Erro: {data['error']}")
        
        elif event_type == "complete":
            # EstatÃ­sticas finais
            print(f"\nğŸ“Š Processamento completo:")
            print(f"  Total: {data['total_files']}")
            print(f"  Sucesso: {data['successful']}")
            print(f"  Falhas: {data['failed']}")
            print(f"  Tempo: {data['processing_time_seconds']:.2f}s")
            print(f"  Labels: {', '.join(data['metadata']['labels_processed'])}")
```

**Como o streaming funciona:**
```
Envio: 2 PDFs "carteira_oab" + 3 PDFs "tela_sistema"

Processamento:
â”œâ”€ Thread 1: carteira_oab (processa sequencialmente)
â”‚   â”œâ”€ oab_1.pdf â†’ ğŸ“¤ SSE evento 1
â”‚   â””â”€ oab_2.pdf â†’ ğŸ“¤ SSE evento 2
â”‚
â””â”€ Thread 2: tela_sistema (processa sequencialmente)
    â”œâ”€ tela_1.pdf â†’ ğŸ“¤ SSE evento 3
    â”œâ”€ tela_2.pdf â†’ ğŸ“¤ SSE evento 4
    â””â”€ tela_3.pdf â†’ ğŸ“¤ SSE evento 5

ğŸ“¤ Evento final: complete

Resultado: Frontend recebe cada arquivo IMEDIATAMENTE apÃ³s processar!
```
---

## ğŸŒ API REST

### Endpoints DisponÃ­veis

#### POST `/extract`
Extrai dados de um PDF individual.

**Request:**
```bash
curl -X POST "http://localhost:8000/extract" \
  -F "file=@documento.pdf" \
  -F "label=carteira_oab" \
  -F 'extraction_schema={"nome":"Nome completo","inscricao":"NÃºmero OAB"}'
```

**Response:**
```json
{
  "success": true,
  "data": {
    "nome": "JoÃ£o Silva",
    "inscricao": "123456"
  },
  "metadata": {
    "method": "llm",
    "time_seconds": 2.341,
    "pipeline_info": {...}
  }
}
```

#### POST `/extract-batch`
Extrai dados de mÃºltiplos PDFs com streaming progressivo (SSE).

Ver exemplo completo em [Processamento em Batch](#-processamento-em-batch-sem-ui).

#### GET `/health`
Verifica saÃºde da API.

```bash
curl http://localhost:8000/health
```

#### GET `/stats`
EstatÃ­sticas detalhadas do sistema.

```bash
curl http://localhost:8000/stats
```

**Response:**
```json
{
  "cache": {
    "l1_size": 42,
    "l1_hits": 158,
    "l1_misses": 23,
    "l2_hits": 12
  },
  "templates": {
    "carteira_oab": 5,
    "tela_sistema": 3
  },
  "extraction_counts": {
    "llm_calls": 31,
    "template_hits": 142,
    "cache_hits": 170
  }
}
```

---

## ğŸ—ï¸ Arquitetura

### Pipeline de ExtraÃ§Ã£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF Input â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Cache L1  â”‚ â”€â”€â”€ Hit? â”€â”€> Retorna (0.1ms)
â”‚    (Memory)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Miss
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Cache L2  â”‚ â”€â”€â”€ Hit? â”€â”€> Retorna (1-2ms)
â”‚    (Disk)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Miss
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Template  â”‚
â”‚    Matching  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Similaridadeâ”‚
  â”‚   >= 90%?  â”‚
  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”‚
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â”‚ SIM     â”‚ NÃƒO
   â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Templateâ”‚ â”‚  LLM   â”‚
â”‚ (0.5s) â”‚ â”‚(2-5s)  â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚          â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 4. Learn â”‚
   â”‚ Template â”‚
   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 5. Cache â”‚
   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Responseâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes

1. **LLM Extractor** (`src/extraction/llm.py`)
   - Modelo: `gpt-5-mini` com structured outputs
   - Parser: `unstructured` com coordenadas espaciais
   - ValidaÃ§Ã£o: Formatos brasileiros (CPF, CEP, telefone, etc.)
   - Timeout: 120s por documento

2. **Cache Manager** (`src/cache/`)
   - L1 (Memory): LRU cache, ~0.1ms
   - L2 (Disk): DiskCache persistente, ~1-2ms
   - Hit rate: 50-90% apÃ³s warm-up

3. **Template Learning** (`src/template/`)
   - Aprende padrÃµes automaticamente de extraÃ§Ãµes LLM
   - Similaridade >= 90% para ativar template
   - ExtraÃ§Ã£o ~10x mais rÃ¡pida que LLM

4. **FastAPI Backend** (`src/main.py`)
   - DocumentaÃ§Ã£o automÃ¡tica (Swagger UI)
   - Health checks e monitoramento
   - Batch processing com streaming

---

## ğŸ“Š Performance

### Benchmarks

| CenÃ¡rio | Tempo | MÃ©todo |
|---------|-------|--------|
| **Primeira extraÃ§Ã£o** | ~3.5s | LLM completo |
| **Cache hit (L1)** | <0.001s | Cache memÃ³ria |
| **Cache hit (L2)** | ~0.001s | Cache disco |
| **Template match (>90%)** | ~0.5s | Template puro |
| **Documento novo** | ~3.5s | LLM completo |

### EvoluÃ§Ã£o com Template Learning

```
Request 1 (doc_1.pdf): LLM    â†’ 3.62s (aprende)
Request 2 (doc_1.pdf): Cache  â†’ 0.2ms (18.000x faster âš¡)
Request 3 (doc_2.pdf): LLM    â†’ 3.41s (aprende)
Request 4 (doc_3.pdf): Template â†’ 0.51s (7x faster âš¡)
Request 5 (doc_2.pdf): Cache  â†’ 0.2ms (cache hit)
```

**ğŸ’¡ Sistema aprende e fica progressivamente mais rÃ¡pido!**

### AcurÃ¡cia

- **MÃ©dia geral**: 89-97%
- **ValidaÃ§Ã£o de formatos **: CEP, CPF, telefone, valores monetÃ¡rios
- **Structured outputs**: Garante JSON vÃ¡lido sempre

---

## ğŸ¯ Tecnologias

- **LLM**: OpenAI GPT-5-mini com structured outputs
- **PDF Processing**: unstructured (coordenadas espaciais)
- **Cache**: diskcache + LRU in-memory
- **Template DB**: SQLite
- **API**: FastAPI + uvicorn
- **Container**: Docker + Docker Compose

---

## ğŸ”§ VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz:

```bash
# ObrigatÃ³rio
OPENAI_API_KEY=sk-proj-...

# Opcionais
PORT=8000
HOST=0.0.0.0
LOG_LEVEL=info
```

---

## ğŸ“ Estrutura do Projeto

```
enter-fellowship/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py              # API FastAPI
â”‚   â”œâ”€â”€ pipeline.py          # Pipeline de extraÃ§Ã£o
â”‚   â”œâ”€â”€ extraction/
â”‚   â”‚   â””â”€â”€ llm.py          # LLM + unstructured
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â”œâ”€â”€ cache_manager.py
â”‚   â”‚   â””â”€â”€ cache_key.py
â”‚   â”œâ”€â”€ template/
â”‚   â”‚   â”œâ”€â”€ template_manager.py
â”‚   â”‚   â”œâ”€â”€ pattern_learner.py
â”‚   â”‚   â”œâ”€â”€ field_extractor.py
â”‚   â”‚   â”œâ”€â”€ template_matcher.py
â”‚   â”‚   â””â”€â”€ database.py
â”‚   â”œâ”€â”€ batch_extract.py     # Script CLI para batch
â”‚   â””â”€â”€ storage/
â”‚       â”œâ”€â”€ cache_data/      # Cache L2
â”‚       â””â”€â”€ templates.db     # Templates aprendidos
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env
```

---

## ğŸ› Troubleshooting

### Docker

**Porta 8000 em uso:**
```bash
# OpÃ§Ã£o 1: Parar processo
lsof -ti:8000 | xargs kill -9

# OpÃ§Ã£o 2: Mudar porta no docker-compose.yml
ports:
  - "8001:8000"
```

**MudanÃ§as nÃ£o refletem:**
```bash
docker compose down
docker compose up -d --build
```

**Erro de permissÃ£o:**
```bash
docker compose down -v
docker compose up -d
```

### API

**Erro 500 ao extrair:**
- Verifique `OPENAI_API_KEY` no `.env`
- Veja logs: `docker compose logs -f api`

**Batch muito lento:**
- Normal na primeira vez (aprende templates)
- Documentos subsequentes serÃ£o mais rÃ¡pidos
- Use `/stats` para ver cache hits

**AcurÃ¡cia baixa:**
- Verifique se schema estÃ¡ bem definido
- Confira qualidade do PDF (OCR pode falhar em PDFs ruins)
- Veja logs de validaÃ§Ã£o para campos especÃ­ficos

---

## ğŸ† Diferenciais

1. **ğŸ¯ Template Learning AutomÃ¡tico**: Aprende com cada extraÃ§Ã£o, fica 7-10x mais rÃ¡pido
2. **âš¡ Streaming Progressivo (SSE)**: Batch com resultados em tempo real
3. **ğŸ’¾ Cache Multi-Level**: <1ms para documentos repetidos
4. **ğŸ“ ValidaÃ§Ã£o BR**: Formatos brasileiros (CPF, CEP, telefone)
5. **ğŸš€ Production-Ready**: Docker, health checks, monitoramento
6. **ğŸ§  Structured Outputs**: JSON vÃ¡lido garantido

---

**Desenvolvido para Enter AI Fellowship** | 2025
